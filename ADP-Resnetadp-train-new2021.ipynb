{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.init as init\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "\n",
    "# 超参数设置\n",
    "EPOCH =200 #遍历数据集次数\n",
    "pre_epoch = 0  # 定义已经遍历数据集的次数\n",
    "BATCH_SIZE =64      #批处理尺寸(batch_size)\n",
    "\n",
    "\n",
    "trainset = datasets.ImageFolder(\n",
    "     '/home/lhl/jupyter_work/abdm/data/ImageNet/train_old',\n",
    "    transforms.Compose([ \n",
    "#              transforms.CenterCrop(224),\n",
    "        transforms.Resize([224,224]),\n",
    "        transforms.ToTensor(),\n",
    "           transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    ")\n",
    "\n",
    "testset = datasets.ImageFolder(\n",
    "\n",
    "                                    '/home/lhl/jupyter_work/abdm/data/ImageNet/test_old' ,\n",
    "            transforms.Compose([ \n",
    "                transforms.Resize([224,224]),\n",
    "        transforms.ToTensor(),\n",
    "                   transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "    ]))\n",
    "# DataLoader实现批量读取\n",
    "trainloader =torch.utils.data.DataLoader(dataset=trainset,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                              num_workers=0,\n",
    "                                    drop_last=True)\n",
    "testloader =torch.utils.data.DataLoader(dataset=testset,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                              num_workers=0,\n",
    "                                       drop_last=True)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(planes)\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ADP(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(ADP, self).__init__()\n",
    "    #         adp_layer\n",
    "    self.conv1_adp = nn.Conv2d(3, 16, kernel_size = 4, stride = 2, padding = 1)\n",
    "    self.BN02_adp =nn.BatchNorm2d(16)\n",
    "    self.conv11_adp = nn.Conv2d(16, 32, kernel_size = 5, stride = 2, padding = 1)\n",
    "    self.BN03_adp =nn.BatchNorm2d(32)\n",
    "    self.conv12_adp = nn.Conv2d(32, 32, kernel_size=5,padding=2)\n",
    "    self.BN04_adp =nn.BatchNorm2d(32)\n",
    "    \n",
    "    self.convo1_adp = nn.Conv2d(32, 64, kernel_size=3)\n",
    "    self.fc1_adp = nn.Linear(1024, 50176)\n",
    "    self.BN0_adp =nn.BatchNorm1d(50176)#224x224\n",
    "    self.longLSTM_adp = nn.LSTMCell(1024,1024) #输入数据列数和隐藏层列数\n",
    "    \n",
    "    self.conv21_adp = nn.Conv2d(3,16, kernel_size=5,padding=2)\n",
    "    self.BN1_adp =nn.BatchNorm2d(16)\n",
    "    self.conv22_adp = nn.Conv2d(16, 32, kernel_size=5,padding=2)\n",
    "    self.BN2_adp =nn.BatchNorm2d(32)\n",
    "    self.conv23_adp = nn.Conv2d(32, 32, kernel_size=3,padding=1)\n",
    "    self.BN22_adp =nn.BatchNorm2d(32)\n",
    "    self.shortLSTM_adp = nn.LSTMCell(512,512) \n",
    "    self.MemorySize_adp=1024\n",
    "\n",
    "  def add_adp(self,x):\n",
    "#     print(\"x\")\n",
    "#     print(x.size())\n",
    "    cuda = torch.cuda.is_available()\n",
    "    if cuda:\n",
    "      longMemory_adp=(Variable(torch.zeros(x.size(0), self.MemorySize_adp).cuda()),\n",
    "            Variable(torch.zeros(x.size(0), self.MemorySize_adp).cuda()))\n",
    "      shortMemory_adp=(Variable(torch.zeros(x.size(0), self.MemorySize_adp).cuda()),\n",
    "            Variable(torch.zeros(x.size(0), self.MemorySize_adp).cuda()))  \n",
    "    else:\n",
    "      longMemory_adp=(Variable(torch.zeros(x.size(0), self.MemorySize_adp)),\n",
    "            Variable(torch.zeros(x.size(0), self.MemorySize_adp)))\n",
    "      shortMemory_adp=(Variable(torch.zeros(x.size(0), self.MemorySize_adp)),\n",
    "            Variable(torch.zeros(x.size(0), self.MemorySize_adp)))\n",
    "    xI=x.clone()\n",
    "    for i in range(4):\n",
    "      x1=x.clone()\n",
    "      xc= F.relu(F.max_pool2d(self.BN02_adp((self.conv1_adp(x1))), 2))\n",
    "      print(\"xc\")\n",
    "      print(xc.size())\n",
    "      xc= F.relu(F.max_pool2d(self.BN03_adp(self.conv11_adp(xc)), 4))\n",
    "      print(\"xc1\")\n",
    "      print(xc.size())\n",
    "      xc= F.relu(self.BN04_adp(self.conv12_adp(xc)))\n",
    "      print(\"xc2\")\n",
    "      print(xc.size())\n",
    "      xc=self.convo1_adp(xc)\n",
    "      print(\"xc3\")\n",
    "      print(xc.size())\n",
    "      xr=xc.view(BATCH_SIZE,1024)#输入lstm的数据纬度\n",
    "      longMemory_adp=self.longLSTM_adp(xr,longMemory_adp) \n",
    "      lt=F.relu(self.BN0_adp(self.fc1_adp(longMemory_adp[0])))\n",
    "\n",
    "      lt=lt.view(-1,224,224)\n",
    "\n",
    "      lt=torch.stack([lt,lt,lt],dim=1) #并成(batchsize,channel,hight,width)\n",
    "\n",
    "      x0=Variable(x.data,requires_grad=False)\n",
    "      xI=torch.mul(lt,x0)                       \n",
    "      xc= F.relu(F.max_pool2d(self.BN1_adp(self.conv21_adp(xI)),2))\n",
    "      xc= F.relu(F.max_pool2d(self.BN2_adp(self.conv22_adp(xc)),2))\n",
    "      xc= F.relu(F.max_pool2d(self.BN22_adp(self.conv23_adp(xc)),2))\n",
    "      xc=xc.view(-1,512)\n",
    "      shortMemory_adp=self.shortLSTM_adp(xc,shortMemory_adp)\n",
    "      longMemory_adp=self.longLSTM_adp(shortMemory_adp[0],longMemory_adp)\n",
    "    return xI\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNetADP(nn.Module):\n",
    "#     num_blocks每一层的blocks数目\n",
    "    def __init__(self, block, num_blocks, num_classes=20):\n",
    "        super(ResNetADP, self).__init__()\n",
    "        self.adp=ADP()\n",
    "        self.in_planes = 64\n",
    "        self.interplanes = [\n",
    "            self.in_planes, self.in_planes * 2, self.in_planes * 4,\n",
    "            self.in_planes * 8\n",
    "        ]\n",
    "        self.conv1 = nn.Conv2d(3,self.in_planes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_planes)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, self.interplanes[0], num_blocks[0], stride=2)\n",
    "        self.layer2 = self._make_layer(block,self.interplanes[1], num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, self.interplanes[2], num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, self.interplanes[3], num_blocks[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.linear = nn.Linear(self.interplanes[3] * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.adp.add_adp(x)\n",
    "#         print(\"out1\")\n",
    "#         print(out.size())\n",
    "        out = F.relu(self.bn1(self.conv1(out)))\n",
    "        out=self.maxpool(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out=self.layer4(out)\n",
    "        out=self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "    def resnet_train(self,device,accuracy):\n",
    "        LR = 0.01        #学习率\n",
    "        initepoch = 0\n",
    "        # 定义损失函数和优化方式\n",
    "        optimizer = optim.SGD(self.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4) #优化方式为mini-batch momentum-SGD，并采用L2正则化（权重衰减）\n",
    "#         print(net)\n",
    "        path = \"./model/Resnet/ResNet50ADP_200s.tar\"\n",
    "        best_acc = 85  #2 初始化best test accuracy\n",
    "        print(\"Start Training ResNetADP\")  # 定义遍历数据集的次数\n",
    "        if os.path.exists(path) is not True:\n",
    "            criterion = nn.CrossEntropyLoss()  #损失函数为交叉熵，多用于多分类问题\n",
    "            optimizer = optim.SGD(self.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4) #优化方式为mini-batch momentum-SGD，并采用L2正则化（权重衰减）\n",
    "        else:\n",
    "            checkpoint = torch.load(path)\n",
    "            self.load_state_dict(checkpoint['model_state_dict'],False)\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'],False)\n",
    "            initepoch = checkpoint['epoch']\n",
    "            criterion = checkpoint['criterion']\n",
    "\n",
    "        for epoch in range(initepoch, EPOCH):\n",
    "            timestart = time.time()\n",
    "            print('\\nEpoch: %d' % (epoch + 1))\n",
    "            net.train()\n",
    "            sum_loss = 0.0\n",
    "            correct = 0.0\n",
    "            total = 0.0\n",
    "            if epoch>=5 and epoch<15:\n",
    "                LR = 0.01\n",
    "                optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4) #优化方式为mini-batch momentum-SGD，并采用L2正则化（权重衰减）\n",
    "\n",
    "            if epoch>=15 and epoch<25:\n",
    "                LR = 0.001\n",
    "                optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4) #优化方式为mini-batch momentum-SGD，并采用L2正则化（权重衰减）\n",
    "                \n",
    "            if epoch>=25 and epoch<35:\n",
    "                LR = 0.0001\n",
    "                optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4) #优化方式为mini-batch momentum-SGD，并采用L2正则化（权重衰减）\n",
    "\n",
    "            if epoch>=35:\n",
    "                LR = 0.0001\n",
    "                optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4) #优化方式为mini-batch momentum-SGD，并采用L2正则化（权重衰减）\n",
    "\n",
    "            print(\"进入数据\")\n",
    "            for i, data in enumerate(trainloader):\n",
    "                # 准备数据\n",
    "                length = len(trainloader)\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                sum_loss += loss.item()\n",
    "            torch.save({'epoch':epoch,\n",
    "                                    'model_state_dict':net.state_dict(),\n",
    "                                    'optimizer_state_dict':optimizer.state_dict(),\n",
    "                                    'criterion':criterion\n",
    "                                    },path)\n",
    "            print('epoch %d cost %3f sec' %(epoch,time.time()-timestart))\n",
    "            print(\"Waiting Test!\")\n",
    "            with torch.no_grad():\n",
    "              correct = 0\n",
    "              total = 0\n",
    "              for data in testloader:\n",
    "                  net.eval()\n",
    "                  images, labels = data\n",
    "                  images, labels = images.to(device), labels.to(device)\n",
    "                  outputs = net(images)\n",
    "                  # 取得分最高的那个类 (outputs.data的索引号)\n",
    "                  _, predicted = torch.max(outputs.data, 1)\n",
    "                  total += labels.size(0)\n",
    "                  correct += (predicted == labels).sum()\n",
    "            accuracy.append(100.0 * correct / total)\n",
    "            print(accuracy)\n",
    "            print('测试分类准确率为：%.3f%%' % (100. * correct / total))\n",
    "            print('Saving model......')\n",
    "     \n",
    "    def resnet_test(self,device,testloader):\n",
    "        print(\"Waiting Test!\")\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for batch_idx, data in enumerate(testloader):\n",
    "                net.eval()\n",
    "                images, labels = data\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = self(images)\n",
    "                # 取得分最高的那个类 (outputs.data的索引号)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum()\n",
    "            print('测试分类准确率为：%.3f%%' % (100 * correct / total))\n",
    "            print(batch_idx)\n",
    "            print(correct)\n",
    "            print(total)\n",
    "            acc = 100. * correct / total\n",
    "            # 将每次测试结果实时写入acc.txt文件中\n",
    "            print('Saving model......',\"correct:\",correct,\" total:\",total)\n",
    "        # print(np.argmax(outputs.cpu()))\n",
    "\n",
    "\n",
    "def ResNet50ADP():\n",
    "    return ResNetADP(Bottleneck, [3,4,6,3])\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = ResNet50ADP()\n",
    "net = net.to(device)\n",
    "accuracy=[]\n",
    "TrainLoss=[]\n",
    "TestLoss=[]\n",
    "net.resnet_train(device,accuracy) #记录训练的loss和测试的loss，记得画折线图\n",
    "net.resnet_test(device,testloader)\n",
    "print(\"done\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abdm_new",
   "language": "python",
   "name": "abdm_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
